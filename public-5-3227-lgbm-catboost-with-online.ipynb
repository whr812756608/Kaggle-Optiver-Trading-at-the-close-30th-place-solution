{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThis notebook is for intorducing how our team achieved 18th on public leaderboard with our **LGBM and CatBoost ensemble model with holdout CV** and demonstrating the process of **online learning to collect public leaderboard data continuosly update the model.** (~40 days after date_id 480). \n\nThese methods are designed for public leaderboard data and should not be extrapolated for private leaderboard predictions. (This is not the notebook we submit for private.)\n\nSpecial thanks to the authors of the following public notebooks for their invaluable contributions:\n\n- **@lblhandsome**, [\"âš¡Optiver ðŸš€Robust Best âš¡ Single Model\"](https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model)\n\n- **@zulqarnainali**, [\"LGB Fine-tuned ðŸš€(Explained)\"](https://www.kaggle.com/code/zulqarnainali/lgb-fine-tuned-explained)\n\n- **@verracodeguacas**, [\"ðŸ–ï¸-Fold CV ðŸš€\"](https://www.kaggle.com/code/verracodeguacas/fold-cv)\n\n- **@yekenot**, [\"Feature Elimination by CatBoost\"](https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost)\n\n- **@jirkaborovec**, [\"ðŸ“ˆOptiverðŸ“‰: Feature Eng & OptunaðŸŒªï¸LightGBM@GPU\"](https://www.kaggle.com/code/jirkaborovec/optiver-feature-eng-optuna-lightgbm-gpu)\n\n- **@yunchonggan**,[\"Weights of the Synthetic Index\"](https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion/442851)\n\n\nI would also like to extend my great appreciation to my teammates for their tremendous effort in this competition:\n\n- **@chellyfan**\n- **@unknownkid**\n\n\n\nPlease note that you should run with **GPU T4x2**. P100 GPU will cause memory issue when training Catboost at Kaggle platform. \nCheck this notebook for detail https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost","metadata":{}},{"cell_type":"markdown","source":"# ðŸ§¹ Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# ðŸ“¦ Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport xgboost as xgb\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport catboost as ctb\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\n# from pandarallel import pandarallel\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# ðŸ¤ Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pandarallel.initialize(nb_workers=4)\n\n# ðŸ“Š Define flags and variables\n# is_offline = True  # Flag for online/offline mode\n# is_train = False  # Flag for training mode\n# is_infer = False  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\nN_STOCKS = 200\nMAX_N_NEIGHBOURS = 10\nNEIGHBOUR_CORR_THRESHOLD = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.075005Z","iopub.execute_input":"2023-12-12T17:18:39.075904Z","iopub.status.idle":"2023-12-12T17:18:39.084333Z","shell.execute_reply.started":"2023-12-12T17:18:39.075868Z","shell.execute_reply":"2023-12-12T17:18:39.083255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“Š Data Loading and Preprocessing ðŸ“Š","metadata":{}},{"cell_type":"code","source":"# ðŸ“‚ Read the dataset from a CSV file using Pandas\n# df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# ðŸ§¹ Remove rows with missing values in the \"target\" column\ndf = df.dropna(subset=[\"target\"])\ndf = df.dropna(subset=[\"wap\"])\n\n# ðŸ” Reset the index of the DataFrame and apply the changes in place\ndf.reset_index(drop=True, inplace=True)\n\n# ðŸ“ Get the shape of the DataFrame (number of rows and columns)\ndf_shape = df.shape\n\ndf_train = df","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.099104Z","iopub.execute_input":"2023-12-12T17:18:39.099508Z","iopub.status.idle":"2023-12-12T17:18:58.440409Z","shell.execute_reply.started":"2023-12-12T17:18:39.099468Z","shell.execute_reply":"2023-12-12T17:18:58.439504Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš€ Memory Optimization Function with Data Type Conversion ðŸ§¹Â¶","metadata":{}},{"cell_type":"code","source":"# ðŸ§¹ Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # ðŸ“ Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # ðŸ”„ Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # â„¹ï¸ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # ðŸ”„ Return the DataFrame with optimized memory usage\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:58.442757Z","iopub.execute_input":"2023-12-12T17:18:58.443076Z","iopub.status.idle":"2023-12-12T17:18:58.458166Z","shell.execute_reply.started":"2023-12-12T17:18:58.443049Z","shell.execute_reply":"2023-12-12T17:18:58.457248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸŽï¸Parallel Triplet Imbalance Calculation with Numba","metadata":{}},{"cell_type":"code","source":"# ðŸŽï¸ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# ðŸ“Š Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # ðŸ” Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # ðŸ” Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # ðŸš« Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n# ðŸ“Š Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # ðŸ” Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # ðŸ” Loop through rows of the DataFrame\n        for j in range(num_rows):\n\n            if df_values[j, a] < df_values[j, b]:\n                min_val = df_values[j, a]\n                max_val = df_values[j, b]\n            else:\n                max_val = df_values[j, a]\n                min_val = df_values[j, b]\n\n            if min_val < df_values[j, c]:\n                if df_values[j, c] < max_val:\n                    mid_val = df_values[j, c]\n                else:\n                    mid_val = max_val\n                    max_val = df_values[j, c]\n            else:\n                mid_val = min_val\n                min_val = df_values[j, c]\n            \n            # ðŸš« Prevent division by zero\n            if max_val == min_val:\n                imbalance_features[j, i] = np.nan\n            elif mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    \n    return imbalance_features\n\n# ðŸ“ˆ Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T23:11:49.791611Z","iopub.execute_input":"2023-12-20T23:11:49.791949Z","iopub.status.idle":"2023-12-20T23:11:50.699673Z","shell.execute_reply.started":"2023-12-20T23:11:49.791924Z","shell.execute_reply":"2023-12-20T23:11:50.698654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“Š Feature Generation Functions ðŸ“Š","metadata":{}},{"cell_type":"code","source":"# ðŸ“Š Function to generate imbalance features\ndef imbalance_features(df):\n\n    stock_groups = df.groupby([\"date_id\", \"seconds_in_bucket\"])\n    # Index WAP\n    df[\"wwap\"] = df.stock_id.map(weights) * df.wap\n    df[\"iwap\"] = stock_groups[\"wwap\"].transform(lambda x: x.sum())\n    del df[\"wwap\"]\n\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"all_size\"] = df.eval(\"matched_size + imbalance_size\") # add\n    df[\"imbalance_size_for_buy_sell\"] = df.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\n    # df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    cols = ['wap', 'imbalance_size_for_buy_sell', \"bid_size\", \"ask_size\"]\n    for q in [0.25, 0.5, 0.75]:  # Try more/different q\n        df[[f'{col}_quantile_{q}' for col in cols]] = stock_groups[cols].transform(lambda x: x.quantile(q)).astype(np.float32)\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\").astype(np.float32)\n\n    for c in combinations(sizes, 2):\n        df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\").astype(np.float32)\n\n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values.astype(np.float32)\n        \n    # V2 features\n    # Calculate additional features\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    df[\"imbalance_momentum\"] = stock_groups['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = stock_groups['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['wap_advantage'] = df.wap - df.iwap  # add\n\n    # Calculate various statistical aggregation features\n    df_prices = df[prices]\n    df_sizes = df[sizes]\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df_prices.agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df_sizes.agg(func, axis=1)\n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    cols = ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_shift_{window}\" for col in cols]] = stock_groups_cols.shift(window)\n\n    cols = ['matched_size', 'imbalance_size', 'reference_price', \"iwap\"] #wap\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_ret_{window}\" for col in cols]] = stock_groups_cols.pct_change(window).astype(np.float32)\n\n    # Calculate diff features for specific columns\n    cols = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price', 'imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_diff_{window}\" for col in cols]] = stock_groups_cols.diff(window).astype(np.float32)\n\n    # V4 features\n    # Construct `time_since_last_imbalance_change`\n    # å½“`imbalance_buy_sell_flag`æ”¹å˜æ—¶ï¼Œ'flag_change'åˆ—ä¼šä¸ºè¯¥è¡Œèµ‹å€¼ä¸º1\n#     df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n#     # ä½¿ç”¨cumsumåˆ›å»ºä¸€ä¸ªç»„æ ‡è¯†ç¬¦ï¼Œæ¯å½“flagæ”¹å˜æ—¶ï¼Œç»„æ ‡è¯†ç¬¦å¢žåŠ \n#     df['group'] = stock_groups['flag_change'].cumsum()\n#     # å¯¹æ¯ä¸ªç»„å†…çš„'seconds_in_bucket'è®¡ç®—æ—¶é—´å·®ï¼Œä»¥å¾—åˆ°è‡ªä¸Šæ¬¡flagæ”¹å˜ä»¥æ¥çš„æ—¶é—´\n#     df['time_since_last_imbalance_change'] = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform(lambda x: x - x.min())\n#     # `flag_change`ä¸º1çš„åœ°æ–¹è®¾ä¸º0\n#     df.loc[df['flag_change'] == 1, 'time_since_last_imbalance_change'] = 0\n#     df.drop(columns=['flag_change', 'group'], inplace=True)\n    \n    df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n    # ä½¿ç”¨cumsumåˆ›å»ºä¸€ä¸ªç»„æ ‡è¯†ç¬¦ï¼Œæ¯å½“flagæ”¹å˜æ—¶ï¼Œç»„æ ‡è¯†ç¬¦å¢žåŠ \n    df['group'] = df.groupby(['stock_id', 'date_id'])['flag_change'].cumsum()\n    # å¯¹æ¯ä¸ªç»„å†…çš„'seconds_in_bucket'è®¡ç®—æ—¶é—´å·®ï¼Œä»¥å¾—åˆ°è‡ªä¸Šæ¬¡flagæ”¹å˜ä»¥æ¥çš„æ—¶é—´\n    group_min = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform('min')\n    df['time_since_last_imbalance_change'] = df['seconds_in_bucket'] - group_min\n    # `flag_change`ä¸º1çš„åœ°æ–¹è®¾ä¸º0\n    df['time_since_last_imbalance_change'] *= (1 - df['flag_change'])\n    df.drop(columns=['flag_change', 'group'], inplace=True)\n    \n    cols = ['imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [5, 10]:\n        mean_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).mean())\n        std_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).std())\n        df[[f'z_score_{col}_{window}' for col in cols]] = (df[cols] - mean_col) / std_col\n    \n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\n# ðŸ“… Function to generate time and stock-related features\ndef other_features(df):\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df[f\"global_seconds_{key}\"] = df[\"seconds_in_bucket\"].map(value.to_dict())\n\n    return df\n\ndef last_days_features(df: pd.DataFrame, feat_last=None, target_last=None):\n    size = None\n    \n    if feat_last is not None and len(feat_last) > 0:\n        cols = [col for col in df.columns if col in set(feat_last.columns)]\n        if target_last is not None:\n            cols.append(\"target\")\n            feat_last[\"target\"] = target_last\n            df[\"target\"] = 0\n        paddings = []\n        second_start = df.seconds_in_bucket.max()\n        padding_src = df[df.seconds_in_bucket == second_start]\n        size = len(df)\n        size_pad = len(padding_src) * 6\n        for second in range(second_start + 10, second_start + 70, 10):\n            padding = padding_src.copy()\n            padding[\"seconds_in_bucket\"] = second\n            paddings.append(padding)\n        df = pd.concat([feat_last[cols], df] + paddings)\n\n    # Add Last days features\n    # TODO: Try more features\n    cols = ['near_price', 'far_price', 'depth_pressure']\n    if 'target' in df.columns:\n        cols.append('target')\n    stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1]:\n        df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n    if cols[-1] == \"target\":\n        cols.pop()\n    \n    cols = [f\"{col}_last_{window}day\" for col in cols]\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6]:\n        df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n        \n    if size:\n        return df[-(size + size_pad):-size_pad]\n    return df\n\n# ðŸš€ Function to generate all features by combining imbalance and other features\ndef generate_all_features(df, feat_last=None, target_last=None):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in {\"row_id\", \"time_id\", \"currently_scored\"}]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    \n    # Generate last days features\n    df = last_days_features(df, feat_last, target_last)\n    \n    # Generate time and stock-related features\n    df = other_features(df)\n\n    gc.collect()  # Perform garbage collection to free up memory\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in {\"row_id\", \"target\", \"time_id\"}]\n    \n    return df[feature_name]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.240287Z","iopub.execute_input":"2023-12-12T17:18:59.240593Z","iopub.status.idle":"2023-12-12T17:18:59.28331Z","shell.execute_reply.started":"2023-12-12T17:18:59.240566Z","shell.execute_reply":"2023-12-12T17:18:59.282367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}\n\nstock_group = df_train.groupby(\"stock_id\")\ndf_train[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\nglobal_stock_id_feats = {\n        \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),\n        \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),\n        \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),\n        \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),\n        \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),\n        \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),\n        \"median_far_price\": stock_group[\"far_price\"].median(),\n        \"median_near_price\": stock_group[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\":stock_group[\"matched_size\"].median(),\n    }\n\nglobal_seconds_feats = {\n    \"median_target\": df_train.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n}\n# stock2nei = get_stock_neighbour(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.284558Z","iopub.execute_input":"2023-12-12T17:18:59.284886Z","iopub.status.idle":"2023-12-12T17:19:06.484298Z","shell.execute_reply.started":"2023-12-12T17:18:59.284859Z","shell.execute_reply":"2023-12-12T17:19:06.483441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM training function\n\nLGBM parameter taken from **@jirkaborovec**, [\"ðŸ“ˆOptiverðŸ“‰: Feature Eng & OptunaðŸŒªï¸LightGBM@GPU\"](https://www.kaggle.com/code/jirkaborovec/optiver-feature-eng-optuna-lightgbm-gpu) except for learning rate and n_estimator","metadata":{}},{"cell_type":"code","source":"def train_lgb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n#     pre = df_train_feats[offline_split][[\"date_id\", \"seconds_in_bucket\"]]\n#     pre[\"target\"] = targets[offline_split].values\n#     df_train_feats[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")\n    # Update global features\n    stock_group = df_train_feats.groupby(\"stock_id\")\n    global global_stock_id_feats, global_seconds_feats\n    global_stock_id_feats = {\n        \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),\n        \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),\n        \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),\n        \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),\n        \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),\n        \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),\n        \"median_far_price\": stock_group[\"far_price\"].median(),\n        \"median_near_price\": stock_group[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\": stock_group[\"matched_size\"].median(),\n    }\n    df_train_feats[\"target\"] = targets.values\n    global_seconds_feats = {\n        \"median_target\": df_train_feats.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n    }\n    del df_train_feats[\"target\"]\n    \n    # update global features\n    for key, value in global_stock_id_feats.items():\n        df_train_feats[f\"global_{key}\"] = df_train_feats[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df_train_feats[f\"global_seconds_{key}\"] = df_train_feats[\"seconds_in_bucket\"].map(value.to_dict())\n        \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5500,\n        \"num_leaves\": 465,\n        \"subsample\": 0.65791,\n        \"colsample_bytree\": 0.7,\n        \"learning_rate\": 0.00877,  # 0.00877\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 14,  # Maximum depth of the tree\n        \"min_child_samples\": 132,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 6,  # L1 regularization term\n        \"reg_lambda\": 0.08,  # L2 regularization term\n    }\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n    \n    train_dataset = lgb.Dataset(df_offline_train[feature_name].values.astype(np.float32), \n                                label=df_offline_train_target.values.astype(np.float32))\n    \n    valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values.astype(np.float32), \n                                label=df_offline_valid_target.values.astype(np.float32))\n    \n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split\n\n    print(\"Valid Model Training.\")\n    \n    # Train a LightGBM model on the offline data\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n  \n    lgb_model.fit(\n        train_dataset.data,\n        train_dataset.label,\n        eval_set=[(valid_dataset.data, valid_dataset.label)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=200),\n            lgb.callback.log_evaluation(period=100),\n        ],\n        feature_name=feature_name,\n    )\n    \n    best_iteration_ = lgb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del lgb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    full_train_dataset = lgb.Dataset(df_train_feats[feature_name].values.astype(np.float32), \n                                     label=df_train_target.values.astype(np.float32))\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = lgb_params.copy()\n    infer_params[\"n_estimators\"] = int(1.2 * best_iteration_)\n    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n    \n    infer_lgb_model.fit(\n        full_train_dataset.data,\n        full_train_dataset.label,\n        feature_name=feature_name,\n        )\n    \n#     infer_lgb_model.fit(\n#         df_train_feats[feature_name].values.astype(np.float32), \n#         df_train_target.values.astype(np.float32), \n#         feature_name = feature_name\n#         )\n\n    return infer_lgb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.49617Z","iopub.execute_input":"2023-12-12T17:19:06.496502Z","iopub.status.idle":"2023-12-12T17:19:06.518112Z","shell.execute_reply.started":"2023-12-12T17:19:06.496475Z","shell.execute_reply":"2023-12-12T17:19:06.517162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost training & feature elinmation \n\nBased on **@yekenot**, [\"Feature Elimination by CatBoost\"](https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost)\n","metadata":{}},{"cell_type":"code","source":"eliminated_features_names = set(['near_price_ask_price_imb',\n                             'iwap',\n                             'all_imbalance_size_for_buy_sell_quantile_0.5',\n                             'iwap_shift_10',\n                             'all_imbalance_size_for_buy_sell_quantile_0.75',\n                             'matched_size_ret_10',\n                             'all_wap_quantile_0.25',\n                             'iwap_shift_6',\n                             'near_price_wap_imb',\n                             'iwap_shift_1',\n                             'iwap_ret_10',\n                             'iwap_ret_6',\n                             'all_imbalance_size_for_buy_sell_quantile_0.25',\n                             'global_ptp_size',\n                             'reference_price_shift_10',\n                             'all_ask_size_quantile_0.75',\n                             'iwap_shift_3',\n                             'iwap_shift_2',\n                             'reference_price_near_price_imb',\n                             'all_bid_size_quantile_0.25',\n                             'global_std_size',\n                             'global_median_price',\n                             'matched_size_ret_6',\n                             'wap_shift_10'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.519207Z","iopub.execute_input":"2023-12-12T17:19:06.519474Z","iopub.status.idle":"2023-12-12T17:19:06.532037Z","shell.execute_reply.started":"2023-12-12T17:19:06.51945Z","shell.execute_reply":"2023-12-12T17:19:06.531179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ctb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    feature_name = [feat for feat in feature_name if feat not in eliminated_features_names]\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n#     target_mean = targets.values.mean()\n#     pre = df_train_feats[offline_split][[\"date_id\", \"seconds_in_bucket\"]]\n#     pre[\"target\"] = targets[offline_split].values\n    \n    ctb_params = dict(iterations=2000,\n                      learning_rate=1.0,\n                      depth=9,\n                      l2_leaf_reg=30,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.66,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=100,\n                      od_type='Iter',\n                      od_wait=200,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      random_strength=4.428571428571429\n                      )\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n#     train_dataset = lgb.Dataset(df_offline_train[feature_name].values, label = df_offline_train_target, feature_name = feature_name)\n#     valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values, label = df_offline_valid_target, feature_name = feature_name)\n\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    ctb_model = ctb.CatBoostRegressor(**ctb_params)\n    \n#   feature elinmation \n\n#     summary = ctb_model.select_features(\n#         df_offline_train[feature_name], df_offline_train_target,\n#         eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n#         features_for_select=feature_name,\n#         num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n#         steps=3,\n#         algorithm=ctb.EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n#         shap_calc_type=ctb.EShapCalcType.Regular,\n#         train_final_model=False,\n#         plot=True,\n#     )\n    ctb_model.fit(\n        df_offline_train[feature_name].astype(np.float32), df_offline_train_target.astype(np.float32),\n        eval_set=[(df_offline_valid[feature_name].astype(np.float32), df_offline_valid_target.astype(np.float32))],\n        use_best_model=True,\n#         early_stopping_rounds=200\n    )\n    \n    best_iteration_ = ctb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split, ctb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = ctb_params.copy()\n    infer_params[\"iterations\"] = int(1.2 * best_iteration_)\n    infer_ctb_model = ctb.CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[feature_name].astype(np.float32), df_train_target.astype(np.float32))\n    return infer_ctb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.533199Z","iopub.execute_input":"2023-12-12T17:19:06.533474Z","iopub.status.idle":"2023-12-12T17:19:06.546761Z","shell.execute_reply.started":"2023-12-12T17:19:06.53345Z","shell.execute_reply":"2023-12-12T17:19:06.545848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference and online learning\n\nTraining the model at date_id 481, 481+15, 481+30 during inferenceing","metadata":{}},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices)/np.sum(std_error)\n    out = prices-std_error*step\n    \n    return out\n\nimport optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\n# train_cache = pd.DataFrame()\nprint(\"Start initialization...\")\ntargets = df_train[\"target\"].astype(np.float32)\ndata = reduce_mem_usage(generate_all_features(df_train))\ndel df_train\ngc.collect()\nprint(\"Initialization done.\")\nstart_date_id = 481  # 481\nstart_pro_date_id = 480\n# online traiing at \ntrain_date_id = set([start_date_id, start_date_id + 15, start_date_id + 30])  # set([start_date_id + 20])\nfeature_name = list(data.columns)\nfeature_name.remove(\"date_id\")\nfeature_name_ctb = [feat for feat in feature_name if feat not in eliminated_features_names]\nfeat_last = None\ntarget_last = None\ntarget_mean = targets.values.mean()\nprint(\"Start prediction...\")\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    currently_scored = test.iloc[0][\"currently_scored\"]\n    date_id = test.iloc[0][\"date_id\"]\n#     currently_scored = date_id >= start_date_id\n    if not currently_scored and date_id != start_pro_date_id:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    del test['currently_scored']\n    seconds_in_bucket = test.iloc[0][\"seconds_in_bucket\"]\n    # Online train\n    if seconds_in_bucket == 0:\n        cache = pd.DataFrame()\n        target_last = revealed_targets[\"revealed_target\"].values.astype(np.float32)\n        #  concat target\n        if len(targets) < len(data):\n            targets = pd.concat([targets, revealed_targets[\"revealed_target\"].astype(np.float32)], ignore_index=True, axis=0)\n        # Online learning\n        if date_id in train_date_id:\n            infer_lgb_model = train_lgb(data, targets, date_id - 1)\n            infer_ctb_model = train_ctb(data, targets, date_id - 1)\n            target_mean = targets.values.mean()\n\n#     now_time = time.time()\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    feat = generate_all_features(cache, feat_last, target_last)\n    if seconds_in_bucket == 540:\n        feat_last = feat.copy()\n        if currently_scored:\n            # concat date for online learning\n            data = pd.concat([data, reduce_mem_usage(feat)], ignore_index=True, axis=0)\n    if not currently_scored:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    feat = feat[-len(test):]\n    lgb_prediction = infer_lgb_model.predict(feat[feature_name])\n    ctb_prediction = infer_ctb_model.predict(feat[feature_name_ctb])\n    prediction = lgb_prediction * 0.67 + ctb_prediction * 0.33\n    prediction = prediction - prediction.mean() + target_mean\n    clipped_predictions = np.clip(prediction, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\nprint(\"OK\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.548125Z","iopub.execute_input":"2023-12-12T17:19:06.548411Z","iopub.status.idle":"2023-12-12T17:25:33.488499Z","shell.execute_reply.started":"2023-12-12T17:19:06.548386Z","shell.execute_reply":"2023-12-12T17:25:33.487471Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
