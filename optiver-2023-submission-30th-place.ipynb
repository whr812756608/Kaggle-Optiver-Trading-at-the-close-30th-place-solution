{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":7241445,"sourceType":"datasetVersion","datasetId":4194257},{"sourceId":7241956,"sourceType":"datasetVersion","datasetId":4194599},{"sourceId":155452817,"sourceType":"kernelVersion"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# 📦 Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport xgboost as xgb\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport catboost as ctb\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\n# from pandarallel import pandarallel\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# 🤐 Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pandarallel.initialize(nb_workers=4)\n\n# 📊 Define flags and variables\n# is_offline = True  # Flag for online/offline mode\n# is_train = False  # Flag for training mode\n# is_infer = False  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\nN_STOCKS = 200\nMAX_N_NEIGHBOURS = 10\nNEIGHBOUR_CORR_THRESHOLD = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:27.778455Z","iopub.execute_input":"2023-12-19T23:08:27.778812Z","iopub.status.idle":"2023-12-19T23:08:27.787697Z","shell.execute_reply.started":"2023-12-19T23:08:27.778786Z","shell.execute_reply":"2023-12-19T23:08:27.786642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# infer_lgb_model = joblib.load(\"/kaggle/input/model-1201-1/lgbm.model\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:27.789506Z","iopub.execute_input":"2023-12-19T23:08:27.789779Z","iopub.status.idle":"2023-12-19T23:08:27.802879Z","shell.execute_reply.started":"2023-12-19T23:08:27.789755Z","shell.execute_reply":"2023-12-19T23:08:27.801935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 📂 Read the dataset from a CSV file using Pandas\n# df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# 🧹 Remove rows with missing values in the \"target\" column\ndf = df.dropna(subset=[\"target\", \"wap\"])\n\n# 🔁 Reset the index of the DataFrame and apply the changes in place\ndf.reset_index(drop=True, inplace=True)\n\n# 📏 Get the shape of the DataFrame (number of rows and columns)\ndf_shape = df.shape\n\ndf_train = df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:27.804039Z","iopub.execute_input":"2023-12-19T23:08:27.80439Z","iopub.status.idle":"2023-12-19T23:08:45.016114Z","shell.execute_reply.started":"2023-12-19T23:08:27.804357Z","shell.execute_reply":"2023-12-19T23:08:45.015288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🧹 Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # 📏 Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # 🔄 Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # ℹ️ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # 🔄 Return the DataFrame with optimized memory usage\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:45.017304Z","iopub.execute_input":"2023-12-19T23:08:45.017588Z","iopub.status.idle":"2023-12-19T23:08:45.031136Z","shell.execute_reply.started":"2023-12-19T23:08:45.017564Z","shell.execute_reply":"2023-12-19T23:08:45.030109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# 📊 Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # 🔁 Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # 🔁 Loop through rows of the DataFrame\n        for j in range(num_rows):\n\n            if df_values[j, a] < df_values[j, b]:\n                min_val = df_values[j, a]\n                max_val = df_values[j, b]\n            else:\n                max_val = df_values[j, a]\n                min_val = df_values[j, b]\n\n            if min_val < df_values[j, c]:\n                if df_values[j, c] < max_val:\n                    mid_val = df_values[j, c]\n                else:\n                    mid_val = max_val\n                    max_val = df_values[j, c]\n            else:\n                mid_val = min_val\n                min_val = df_values[j, c]\n            \n            # 🚫 Prevent division by zero\n            if max_val == min_val:\n                imbalance_features[j, i] = np.nan\n            elif mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    \n    return imbalance_features\n\n# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:45.034623Z","iopub.execute_input":"2023-12-19T23:08:45.034998Z","iopub.status.idle":"2023-12-19T23:08:45.931417Z","shell.execute_reply.started":"2023-12-19T23:08:45.034963Z","shell.execute_reply":"2023-12-19T23:08:45.930577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 📊 Function to generate imbalance features\ndef imbalance_features(df: pd.DataFrame):\n\n    stock_groups = df.groupby([\"date_id\", \"seconds_in_bucket\"])\n    # Index WAP\n    df[\"wwap\"] = df.stock_id.map(weights) * df.wap\n    df[\"iwap\"] = stock_groups[\"wwap\"].transform(lambda x: x.sum())\n    del df[\"wwap\"]\n\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"all_size\"] = df.eval(\"matched_size + imbalance_size\")  # add\n    df[\"imbalance_size_for_buy_sell\"] = df.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\n\n    cols = ['wap', 'imbalance_size_for_buy_sell', \"bid_size\", \"ask_size\"]\n    for q in [0.25, 0.5, 0.75]:  # Try more/different q\n        df[[f'all_{col}_quantile_{q}' for col in cols]] = stock_groups[cols].transform(lambda x: x.quantile(q))\n\n    df[\"1/bid_size\"]= 1/df.bid_size\n    df[\"1/sqrt(bid_size)\"] = 1/np.sqrt(df.bid_size)\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in combinations(sizes, 2):\n        df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\")\n\n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n        \n    # V2 features\n    # Calculate additional features\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    df[\"imbalance_momentum\"] = stock_groups['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = stock_groups['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * df[\"price_spread\"]\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['wap_advantage'] = df.wap - df.iwap  # add\n    \n    # Calculate various statistical aggregation features\n    df_prices = df[prices]\n    df_sizes = df[sizes]\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df_prices.agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df_sizes.agg(func, axis=1)\n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    cols = ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_shift_{window}\" for col in cols]] = stock_groups_cols.shift(window)\n\n    cols = ['matched_size', 'imbalance_size', 'reference_price', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_ret_{window}\" for col in cols]] = stock_groups_cols.pct_change(window)\n\n    # Calculate diff features for specific columns\n    cols = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price', 'imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_diff_{window}\" for col in cols]] = stock_groups_cols.diff(window)\n\n    # V4 features\n    df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n    # 使用cumsum创建一个组标识符，每当flag改变时，组标识符增加\n    df['group'] = df.groupby(['stock_id', 'date_id'])['flag_change'].cumsum()\n    # 对每个组内的'seconds_in_bucket'计算时间差，以得到自上次flag改变以来的时间\n    group_min = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform('min')\n    df['time_since_last_imbalance_change'] = df['seconds_in_bucket'] - group_min\n    # `flag_change`为1的地方设为0\n    df['time_since_last_imbalance_change'] *= (1 - df['flag_change'])\n    df.drop(columns=['flag_change', 'group'], inplace=True)\n\n    # V5 features\n    cols = ['imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [5, 10]:\n        mean_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).mean())\n        std_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).std())\n        df[[f'z_score_{col}_{window}' for col in cols]] = (df[cols] - mean_col) / std_col\n    \n    # ------\n    \n    combin = [['ask_price','wap'],['ask_size','imbalance_size'],['bid_price','wap'],['bid_size','ask_size'],['bid_size','imbalance_size'],\n    ['matched_size','imbalance_size'],['matched_size','ask_size'],['matched_size','bid_size'],['reference_price','ask_price'],['reference_price','bid_price'],\n    ['reference_price','imbalance_size_for_buy_sell'],['reference_price','wap'],['wap','imbalance_size_for_buy_sell'],['wap','iwap']]\n    for c in combin:\n        spread_col = f'{c[0]}_{c[1]}_spread'\n        df[spread_col] = df[c[0]] - df[c[1]]\n        df[f'{spread_col}_diff'] = stock_groups[spread_col].diff(1)\n        del df[spread_col]  # Delete the temporary column to save memory\n\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    df['ask_size_mean'] = df.groupby([\"date_id\", \"seconds_in_bucket\"])['ask_size'].transform(lambda x: x.mean())\n    df['wap_std'] = df.groupby([\"date_id\", \"seconds_in_bucket\"])['wap'].transform(lambda x: x.std())\n\n    df['ask_value'] = df.eval('ask_price * ask_size')\n    df['bid_value'] = df.eval('bid_price * bid_size')\n    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x : 1 if x > 0 else (-1 if x < 0 else 0))\n    df[\"my_target\"] = (df[\"wap_ret_6\"] - df[\"iwap_ret_6\"]) * 10000\n    df[\"my_target_abs_mean\"] = df.groupby(['date_id', 'seconds_in_bucket'])[\"my_target\"].transform(lambda x: x.abs().mean())\n    del df[\"my_target\"]\n\n    df['order_flow'] = df['bid_size'] - df['ask_size']\n    df['size_spread'] = df.eval(\"bid_size - ask_size\")\n    df[\"reference_price_momentum\"] = stock_groups['reference_price'].diff(periods=1) / df['matched_size']\n    df['relative_matched_size'] = df['matched_size'] / (df['ask_size'] + df['bid_size'])\n    df['spread_roc'] = stock_groups['price_spread'].pct_change()\n    df['spread_weighted_by_imbalance'] = df['price_spread'] * df['imbalance_buy_sell_flag']\n    df['stock_weight'] = df.stock_id.map(weights)\n    df['weight_sum'] = df.groupby([\"date_id\", \"seconds_in_bucket\"])['stock_weight'].transform(lambda x: x.sum())\n    df[\"w_iwap\"] =  df[\"iwap\"] / df['weight_sum']   # add\n    del df['weight_sum'], df['stock_weight']\n\n    df['w_iwap_advantage'] = df['wap'] - df['w_iwap'] #add\n\n    df['rolling_kurt_5_wap'] = stock_groups['wap'].transform(lambda x: x.rolling(5).agg('kurt'))\n    df['rolling_skew_5_wap'] = stock_groups['wap'].transform(lambda x: x.rolling(5).agg('skew'))\n    df['rolling_kurt_5_reference'] = stock_groups['reference_price'].transform(lambda x: x.rolling(5).agg('kurt'))\n    df['rolling_std_30_reference'] = stock_groups['reference_price'].transform(lambda x: x.rolling(30).agg('std'))\n    df['rolling_skew_5_reference'] = stock_groups['reference_price'].transform(lambda x: x.rolling(5).agg('skew'))\n    df['rolling_kurt_30_reference'] = stock_groups['reference_price'].transform(lambda x: x.rolling(30).agg('kurt'))\n\n    # MACD\n    cols = [\"far_price\", \"near_price\", \"reference_price\", \"wap\"]\n    stock_groups_cols= stock_groups[cols]\n    df[[f'{col}_ema_12' for col in cols]] = stock_groups_cols.transform(lambda x: x.ewm(span=12, adjust=False).mean())\n    df[[f'{col}_ema_26' for col in cols]] = stock_groups_cols.transform(lambda x: x.ewm(span=26, adjust=False).mean())\n    df[[f'{col}_macd' for col in cols]] = df[[f'{col}_ema_12' for col in cols]].values - df[[f'{col}_ema_26' for col in cols]].values\n    df[[f\"{col}_macd_signal\" for col in cols]] = stock_groups[[f'{col}_macd' for col in cols]].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\n# 📅 Function to generate time and stock-related features\ndef other_features(df):\n    # df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    df[\"stock_id&seconds_in_bucket\"] = df['stock_id'].astype(str) + '_' + df['seconds_in_bucket'].astype(str)\n    for key, value in global_seconds_feats_stock.items():\n        df[f\"global_seconds_{key}_stock\"] = df[\"stock_id&seconds_in_bucket\"].map(value.to_dict())\n    del df[\"stock_id&seconds_in_bucket\"]\n\n    return df\n\ndef last_days_features(df: pd.DataFrame, feat_last=None, target_last=None):\n    size = None\n    if feat_last is not None and len(feat_last) > 0:\n        cols = [col for col in df.columns if col in set(feat_last.columns)]\n        if target_last is not None:\n            cols.append(\"target\")\n            feat_last[\"target\"] = target_last\n            df[\"target\"] = 0\n        paddings = []\n        second_start = df.seconds_in_bucket.max()\n        padding_src = df[df.seconds_in_bucket == second_start]\n        size = len(df)\n        size_pad = len(padding_src) * 6\n        for second in range(second_start + 10, second_start + 70, 10):\n            padding = padding_src.copy()\n            padding[\"seconds_in_bucket\"] = second\n            paddings.append(padding)\n        df = pd.concat([feat_last[cols], df] + paddings)\n\n    # Add Last days features\n    cols = ['near_price', 'far_price', 'depth_pressure']\n    stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1]:  # Only [1] is enough\n        df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n    \n    cols = [f\"{col}_last_{window}day\" for col in cols for window in [1]]\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6]:\n        df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n\n    if 'target' in df.columns:\n        cols = ['target']\n        stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n        stock_groups_cols = stock_groups[cols]\n        for window in [1]:  # Only [1] is enough\n            df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n\n        cols = [f\"{col}_last_{window}day\" for col in cols for window in [1]]\n        stock_groups = df.groupby(['stock_id', 'date_id'])\n        stock_groups_cols = stock_groups[cols]\n        for window in [1, 2]:\n            df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n\n    if size:\n        return df[-(size + size_pad):-size_pad]\n    return df\n\n# 🚀 Function to generate all features by combining imbalance and other features\ndef generate_all_features(df, feat_last=None, target_last=None):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in {\"row_id\", \"time_id\"}]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    \n    # Generate last days features\n    df = last_days_features(df, feat_last, target_last)\n\n    # Generate time and stock-related features\n    df = other_features(df)\n    \n    gc.collect()  # Perform garbage collection to free up memory\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in {\"row_id\", \"target\", \"time_id\"}]\n    \n    return df[feature_name]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:45.933014Z","iopub.execute_input":"2023-12-19T23:08:45.93332Z","iopub.status.idle":"2023-12-19T23:08:45.987571Z","shell.execute_reply.started":"2023-12-19T23:08:45.933294Z","shell.execute_reply":"2023-12-19T23:08:45.986724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}\n\ndf_train[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\ndf_train[\"depth_pressure\"] = df_train[\"far_price\"] - df_train[\"near_price\"]\nglobal_stock_id_feats = {\n    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n\n    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    \"median_far_price\": df_train.groupby(\"stock_id\")[\"far_price\"].median(),\n    \"median_near_price\": df_train.groupby(\"stock_id\")[\"near_price\"].median(),\n    \"median_imbalance_size_for_buy_sell\": df_train.groupby(\"stock_id\")[\"imbalance_size_for_buy_sell\"].median(),\n    \"median_matched_size\": df_train.groupby(\"stock_id\")[\"matched_size\"].median(),\n    \"median_depth_pressure\": df_train.groupby(\"stock_id\")[\"depth_pressure\"].median(),\n}\n\ndf_train[\"stock_id&seconds_in_bucket\"] = df_train['stock_id'].astype(str) + '_' + df_train['seconds_in_bucket'].astype(str)\nglobal_seconds_feats_stock = {\n    \"median_imbalance_size_for_buy_sell\": df_train.groupby(\"stock_id&seconds_in_bucket\")[\"imbalance_size_for_buy_sell\"].median(),\n    \"median_matched_size\": df_train.groupby(\"stock_id&seconds_in_bucket\")[\"matched_size\"].median(),\n}\ndel df_train[\"stock_id&seconds_in_bucket\"], df_train[\"imbalance_size_for_buy_sell\"], df_train[\"depth_pressure\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:45.988757Z","iopub.execute_input":"2023-12-19T23:08:45.989056Z","iopub.status.idle":"2023-12-19T23:08:57.808751Z","shell.execute_reply.started":"2023-12-19T23:08:45.989032Z","shell.execute_reply":"2023-12-19T23:08:57.807925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def golden_section_search(f, a, b, epsilon):\n    phi = (1 + 5 ** 0.5) / 2  # golden ratio\n    c = b - (b - a) / phi\n    d = a + (b - a) / phi\n\n    while abs(b - a) > epsilon:\n        if f(c) < f(d):\n            b = d\n        else:\n            a = c\n        c = b - (b - a) / phi\n        d = a + (b - a) / phi\n\n    return (b + a) / 2","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.809915Z","iopub.execute_input":"2023-12-19T23:08:57.810212Z","iopub.status.idle":"2023-12-19T23:08:57.816615Z","shell.execute_reply.started":"2023-12-19T23:08:57.810186Z","shell.execute_reply":"2023-12-19T23:08:57.815581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name_lgb = \\\n['stock_id',\n 'seconds_in_bucket',\n 'imbalance_size',\n 'imbalance_buy_sell_flag',\n 'reference_price',\n 'matched_size',\n 'far_price',\n 'near_price',\n 'bid_price',\n 'bid_size',\n 'ask_price',\n 'ask_size',\n 'wap',\n 'iwap',\n 'volume',\n 'mid_price',\n 'liquidity_imbalance',\n 'matched_imbalance',\n 'all_size',\n 'imbalance_size_for_buy_sell',\n 'all_wap_quantile_0.25',\n 'all_imbalance_size_for_buy_sell_quantile_0.25',\n 'all_bid_size_quantile_0.25',\n 'all_ask_size_quantile_0.25',\n 'all_wap_quantile_0.5',\n 'all_imbalance_size_for_buy_sell_quantile_0.5',\n 'all_bid_size_quantile_0.5',\n 'all_ask_size_quantile_0.5',\n 'all_wap_quantile_0.75',\n 'all_imbalance_size_for_buy_sell_quantile_0.75',\n 'all_bid_size_quantile_0.75',\n 'all_ask_size_quantile_0.75',\n 'reference_price_far_price_imb',\n 'reference_price_near_price_imb',\n 'reference_price_ask_price_imb',\n 'reference_price_bid_price_imb',\n 'reference_price_wap_imb',\n 'far_price_near_price_imb',\n 'far_price_ask_price_imb',\n 'far_price_bid_price_imb',\n 'far_price_wap_imb',\n 'near_price_ask_price_imb',\n 'near_price_bid_price_imb',\n 'near_price_wap_imb',\n 'ask_price_bid_price_imb',\n 'ask_price_wap_imb',\n 'bid_price_wap_imb',\n 'matched_size/bid_size',\n 'matched_size/ask_size',\n 'matched_size/imbalance_size',\n 'bid_size/ask_size',\n 'bid_size/imbalance_size',\n 'ask_size/imbalance_size',\n 'ask_price_bid_price_wap_imb2',\n 'ask_price_bid_price_reference_price_imb2',\n 'ask_price_wap_reference_price_imb2',\n 'bid_price_wap_reference_price_imb2',\n 'matched_size_bid_size_ask_size_imb2',\n 'matched_size_bid_size_imbalance_size_imb2',\n 'matched_size_ask_size_imbalance_size_imb2',\n 'bid_size_ask_size_imbalance_size_imb2',\n 'imbalance_momentum',\n 'price_spread',\n 'spread_intensity',\n 'price_pressure',\n 'market_urgency',\n 'depth_pressure',\n 'wap_advantage',\n 'all_prices_mean',\n 'all_sizes_mean',\n 'all_prices_std',\n 'all_sizes_std',\n 'all_prices_skew',\n 'all_sizes_skew',\n 'all_prices_kurt',\n 'all_sizes_kurt',\n 'matched_size_shift_1',\n 'imbalance_size_shift_1',\n 'reference_price_shift_1',\n 'imbalance_buy_sell_flag_shift_1',\n 'wap_shift_1',\n 'iwap_shift_1',\n 'matched_size_shift_2',\n 'imbalance_size_shift_2',\n 'reference_price_shift_2',\n 'imbalance_buy_sell_flag_shift_2',\n 'wap_shift_2',\n 'iwap_shift_2',\n 'matched_size_shift_3',\n 'imbalance_size_shift_3',\n 'reference_price_shift_3',\n 'imbalance_buy_sell_flag_shift_3',\n 'wap_shift_3',\n 'iwap_shift_3',\n 'matched_size_shift_6',\n 'imbalance_size_shift_6',\n 'reference_price_shift_6',\n 'imbalance_buy_sell_flag_shift_6',\n 'wap_shift_6',\n 'iwap_shift_6',\n 'matched_size_shift_10',\n 'imbalance_size_shift_10',\n 'reference_price_shift_10',\n 'imbalance_buy_sell_flag_shift_10',\n 'wap_shift_10',\n 'iwap_shift_10',\n 'matched_size_ret_1',\n 'imbalance_size_ret_1',\n 'reference_price_ret_1',\n 'wap_ret_1',\n 'iwap_ret_1',\n 'matched_size_ret_2',\n 'imbalance_size_ret_2',\n 'reference_price_ret_2',\n 'wap_ret_2',\n 'iwap_ret_2',\n 'matched_size_ret_3',\n 'imbalance_size_ret_3',\n 'reference_price_ret_3',\n 'wap_ret_3',\n 'iwap_ret_3',\n 'matched_size_ret_6',\n 'imbalance_size_ret_6',\n 'reference_price_ret_6',\n 'wap_ret_6',\n 'iwap_ret_6',\n 'matched_size_ret_10',\n 'imbalance_size_ret_10',\n 'reference_price_ret_10',\n 'wap_ret_10',\n 'iwap_ret_10',\n 'ask_price_diff_1',\n 'bid_price_diff_1',\n 'ask_size_diff_1',\n 'bid_size_diff_1',\n 'wap_diff_1',\n 'near_price_diff_1',\n 'far_price_diff_1',\n 'imbalance_size_for_buy_sell_diff_1',\n 'ask_price_diff_2',\n 'bid_price_diff_2',\n 'ask_size_diff_2',\n 'bid_size_diff_2',\n 'wap_diff_2',\n 'near_price_diff_2',\n 'far_price_diff_2',\n 'imbalance_size_for_buy_sell_diff_2',\n 'ask_price_diff_3',\n 'bid_price_diff_3',\n 'ask_size_diff_3',\n 'bid_size_diff_3',\n 'wap_diff_3',\n 'near_price_diff_3',\n 'far_price_diff_3',\n 'imbalance_size_for_buy_sell_diff_3',\n 'ask_price_diff_6',\n 'bid_price_diff_6',\n 'ask_size_diff_6',\n 'bid_size_diff_6',\n 'wap_diff_6',\n 'near_price_diff_6',\n 'far_price_diff_6',\n 'imbalance_size_for_buy_sell_diff_6',\n 'ask_price_diff_10',\n 'bid_price_diff_10',\n 'ask_size_diff_10',\n 'bid_size_diff_10',\n 'wap_diff_10',\n 'near_price_diff_10',\n 'far_price_diff_10',\n 'imbalance_size_for_buy_sell_diff_10',\n 'time_since_last_imbalance_change',\n 'z_score_imbalance_size_for_buy_sell_5',\n 'z_score_imbalance_size_for_buy_sell_10',\n 'near_price_last_1day',\n 'far_price_last_1day',\n 'depth_pressure_last_1day',\n 'target_last_1day',\n 'near_price_last_1day_future_1',\n 'far_price_last_1day_future_1',\n 'depth_pressure_last_1day_future_1',\n 'near_price_last_1day_future_2',\n 'far_price_last_1day_future_2',\n 'depth_pressure_last_1day_future_2',\n 'near_price_last_1day_future_3',\n 'far_price_last_1day_future_3',\n 'depth_pressure_last_1day_future_3',\n 'near_price_last_1day_future_6',\n 'far_price_last_1day_future_6',\n 'depth_pressure_last_1day_future_6',\n 'seconds',\n 'minute',\n 'global_median_size',\n 'global_std_size',\n 'global_ptp_size',\n 'global_median_price',\n 'global_std_price',\n 'global_ptp_price',\n 'global_median_far_price',\n 'global_median_near_price',\n 'global_median_imbalance_size_for_buy_sell',\n 'global_median_matched_size']","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.818083Z","iopub.execute_input":"2023-12-19T23:08:57.818643Z","iopub.status.idle":"2023-12-19T23:08:57.832953Z","shell.execute_reply.started":"2023-12-19T23:08:57.818616Z","shell.execute_reply":"2023-12-19T23:08:57.832081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name_ctb = ['seconds_in_bucket',\n 'imbalance_size',\n 'matched_size',\n 'bid_size',\n 'ask_price',\n 'ask_size',\n 'wap',\n 'volume',\n 'liquidity_imbalance',\n 'matched_imbalance',\n 'all_size',\n 'imbalance_size_for_buy_sell',\n 'imbalance_momentum',\n 'price_pressure',\n 'market_urgency',\n 'depth_pressure',\n 'size_imbalance',\n '1/bid_size',\n '1/sqrt(bid_size)',\n 'rolling_skew_5_wap',\n 'rolling_skew_5_reference',\n 'rolling_kurt_5_wap',\n 'rolling_kurt_5_reference',\n 'rolling_std_30_reference',\n 'rolling_kurt_30_reference',\n 'size_spread',\n 'bid_value',\n 'ask_value',\n 'mid_price_movement',\n 'order_flow',\n 'reference_price_momentum',\n 'spread_roc',\n 'relative_matched_size',\n 'spread_weighted_by_imbalance',\n 'w_iwap_advantage',\n 'reference_price_bid_price_spread_diff',\n 'reference_price_ask_price_spread_diff',\n 'reference_price_wap_spread_diff',\n 'reference_price_imbalance_size_for_buy_sell_spread_diff',\n 'bid_price_wap_spread_diff',\n 'ask_price_wap_spread_diff',\n 'wap_iwap_spread_diff',\n 'wap_imbalance_size_for_buy_sell_spread_diff',\n 'matched_size_bid_size_spread_diff',\n 'matched_size_ask_size_spread_diff',\n 'matched_size_imbalance_size_spread_diff',\n 'bid_size_ask_size_spread_diff',\n 'bid_size_imbalance_size_spread_diff',\n 'ask_size_imbalance_size_spread_diff',\n 'all_ask_size_quantile_0.5',\n 'all_bid_size_quantile_0.75',\n 'all_ask_size_quantile_0.75',\n 'ask_size_mean',\n 'wap_std',\n 'reference_price_near_price_imb',\n 'reference_price_ask_price_imb',\n 'reference_price_bid_price_imb',\n 'reference_price_wap_imb',\n 'bid_price_wap_imb',\n 'matched_size/bid_size',\n 'matched_size/ask_size',\n 'matched_size/imbalance_size',\n 'bid_size/ask_size',\n 'ask_price_bid_price_wap_imb2',\n 'ask_price_bid_price_reference_price_imb2',\n 'matched_size_bid_size_ask_size_imb2',\n 'matched_size_bid_size_imbalance_size_imb2',\n 'matched_size_ask_size_imbalance_size_imb2',\n 'bid_size_ask_size_imbalance_size_imb2',\n 'all_sizes_skew',\n 'all_sizes_kurt',\n 'matched_size_shift_1',\n 'imbalance_size_shift_1',\n 'reference_price_shift_1',\n 'imbalance_buy_sell_flag_shift_1',\n 'matched_size_shift_2',\n 'imbalance_size_shift_2',\n 'imbalance_buy_sell_flag_shift_2',\n 'wap_shift_2',\n 'matched_size_shift_3',\n 'reference_price_shift_3',\n 'imbalance_buy_sell_flag_shift_3',\n 'wap_shift_3',\n 'matched_size_shift_6',\n 'imbalance_buy_sell_flag_shift_6',\n 'wap_shift_6',\n 'imbalance_buy_sell_flag_shift_10',\n 'matched_size_ret_1',\n 'imbalance_size_ret_1',\n 'iwap_ret_1',\n 'imbalance_size_ret_2',\n 'reference_price_ret_2',\n 'wap_ret_2',\n 'iwap_ret_2',\n 'matched_size_ret_3',\n 'imbalance_size_ret_3',\n 'reference_price_ret_3',\n 'iwap_ret_3',\n 'imbalance_size_ret_6',\n 'reference_price_ret_6',\n 'imbalance_size_ret_10',\n 'ask_price_diff_1',\n 'bid_price_diff_1',\n 'ask_size_diff_1',\n 'bid_size_diff_1',\n 'near_price_diff_1',\n 'imbalance_size_for_buy_sell_diff_1',\n 'ask_size_diff_2',\n 'bid_size_diff_2',\n 'wap_diff_2',\n 'near_price_diff_2',\n 'imbalance_size_for_buy_sell_diff_2',\n 'ask_price_diff_3',\n 'bid_size_diff_3',\n 'wap_diff_3',\n 'near_price_diff_3',\n 'imbalance_size_for_buy_sell_diff_3',\n 'ask_size_diff_6',\n 'bid_size_diff_6',\n 'wap_diff_6',\n 'near_price_diff_6',\n 'imbalance_size_for_buy_sell_diff_6',\n 'ask_size_diff_10',\n 'imbalance_size_for_buy_sell_diff_10',\n 'z_score_imbalance_size_for_buy_sell_5',\n 'z_score_imbalance_size_for_buy_sell_10',\n 'harmonic_imbalance',\n 'my_target_abs_mean',\n 'target_last_1day',\n 'near_price_last_1day_future_6',\n 'target_last_1day_future_1',\n 'target_last_1day_future_2',\n 'seconds',\n 'minute',\n 'global_median_size',\n 'global_median_far_price',\n 'global_median_imbalance_size_for_buy_sell',\n 'global_median_matched_size',\n 'global_median_depth_pressure',\n 'global_seconds_median_imbalance_size_for_buy_sell_stock',\n 'global_seconds_median_matched_size_stock',\n ]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.834231Z","iopub.execute_input":"2023-12-19T23:08:57.834797Z","iopub.status.idle":"2023-12-19T23:08:57.852361Z","shell.execute_reply.started":"2023-12-19T23:08:57.834746Z","shell.execute_reply":"2023-12-19T23:08:57.851501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_lgb(df_train_feats, targets, split_day):\n    \n    # stress test\n#     split_day = df_train_feats.date_id.max()\n    target_mean = targets.values.mean()\n    \n    # Update global features\n    global global_stock_id_feats, global_seconds_feats_stock, feature_name_lgb\n    global_stock_id_feats = {\n        \"median_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].median() + df_train_feats.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].std() + df_train_feats.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].max() - df_train_feats.groupby(\"stock_id\")[\"bid_size\"].min(),\n\n        \"median_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].median() + df_train_feats.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].std() + df_train_feats.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].max() - df_train_feats.groupby(\"stock_id\")[\"ask_price\"].min(),\n        \"median_far_price\": df_train_feats.groupby(\"stock_id\")[\"far_price\"].median(),\n        \"median_near_price\": df_train_feats.groupby(\"stock_id\")[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": df_train_feats.groupby(\"stock_id\")[\"imbalance_size_for_buy_sell\"].median(),\n        \"median_matched_size\": df_train_feats.groupby(\"stock_id\")[\"matched_size\"].median(),\n        \"median_depth_pressure\": df_train_feats.groupby(\"stock_id\")[\"depth_pressure\"].median(),\n    }\n\n    df_train_feats[\"stock_id&seconds_in_bucket\"] = df_train_feats['stock_id'].astype(str) + '_' + df_train_feats['seconds_in_bucket'].astype(str)\n    global_seconds_feats_stock = {\n        \"median_imbalance_size_for_buy_sell\": df_train_feats.groupby(\"stock_id&seconds_in_bucket\")[\"imbalance_size_for_buy_sell\"].median(),\n        \"median_matched_size\": df_train_feats.groupby(\"stock_id&seconds_in_bucket\")[\"matched_size\"].median(),\n    }\n    \n    # update global features\n    for key, value in global_stock_id_feats.items():\n        df_train_feats[f\"global_{key}\"] = df_train_feats[\"stock_id\"].map(value.to_dict())\n\n    for key, value in global_seconds_feats_stock.items():\n        df_train_feats[f\"global_seconds_{key}_stock\"] = df_train_feats[\"stock_id&seconds_in_bucket\"].map(value.to_dict())\n    del df_train_feats[\"stock_id&seconds_in_bucket\"]\n        \n    # Limit N newest data to avoid OOM\n#     keep_days = 550\n#     data_mask = df_train_feats.date_id >= split_day + 1 - keep_days\n# #     keep_rows = 6274400 # 600: 6557200; 575: 6274400\n#     keep_rows = data_mask.sum()\n#     df_train_feats = df_train_feats[-keep_rows:]\n#     targets = targets[-keep_rows:]\n#     del data_mask\n#     gc.collect()\n\n#     feature_name = list(df_train_feats.columns)\n#     feature_name.remove(\"date_id\")\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n    pre = df_offline_valid[[\"date_id\", \"seconds_in_bucket\"]]\n    pre[\"target\"] = df_offline_valid_target.values\n        \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5000,\n        \"num_leaves\": 465,\n        \"subsample\": 0.65791,\n        \"colsample_bytree\": 0.7,\n        \"learning_rate\": 0.00877,  # 0.00877\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 14,  # Maximum depth of the tree\n        \"min_child_samples\": 132,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 6,  # L1 regularization term\n        \"reg_lambda\": 0.08,  # L2 regularization term\n    }\n\n    print(f\"Feature length = {len(feature_name_lgb)}\")\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_offline_train[feature_name_lgb].values.astype(np.float32),\n        df_offline_train_target.values.astype(np.float32),\n        eval_set=[(df_offline_valid[feature_name_lgb].values.astype(np.float32), df_offline_valid_target.values.astype(np.float32))],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=200),\n            lgb.callback.log_evaluation(period=100),\n        ],\n        feature_name = feature_name_lgb\n    )\n    \n    best_iteration_ = lgb_model.best_iteration_\n    \n    pre[\"target_pre\"] = lgb_model.predict(df_offline_valid[feature_name_lgb])\n    pre[\"target_pre_pro\"] = pre.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target_pre\"].transform(lambda x: x - x.mean() + target_mean)\n    pre_lgb = pre[\"target_pre_pro\"].values\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split, lgb_model, pre\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = lgb_params.copy()\n    infer_params[\"n_estimators\"] = int(1.2 * best_iteration_)\n    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n    infer_lgb_model.fit(\n        df_train_feats[feature_name_lgb].values.astype(np.float32), \n        df_train_target.values.astype(np.float32), \n        feature_name = feature_name_lgb\n        )\n    print(\"LightGBM training completed.\")\n    return infer_lgb_model, pre_lgb","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.85384Z","iopub.execute_input":"2023-12-19T23:08:57.854082Z","iopub.status.idle":"2023-12-19T23:08:57.875323Z","shell.execute_reply.started":"2023-12-19T23:08:57.85406Z","shell.execute_reply":"2023-12-19T23:08:57.87441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ctb(df_train_feats, targets, split_day):\n    \n    # stress test\n#     split_day = df_train_feats.date_id.max()\n    target_mean = targets.values.mean()\n    \n#     feature_name = list(df_train_feats.columns)\n#     feature_name.remove(\"date_id\")\n#     feature_name = [feat for feat in feature_name if feat not in eliminated_features_names]\n    global feature_name_ctb\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n    pre = df_offline_valid[[\"date_id\", \"seconds_in_bucket\"]]\n    pre[\"target\"] = df_offline_valid_target.values\n    \n    ctb_params = dict(iterations=2000,\n                      learning_rate=1.0,\n                      depth=9,\n                      l2_leaf_reg=30,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.66,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=100,\n                      od_type='Iter',\n                      od_wait=200,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      random_strength=4.428571428571429\n                      )\n\n    print(f\"Feature length = {len(feature_name_ctb)}\")\n    \n#     train_dataset = lgb.Dataset(df_offline_train[feature_name].values, label = df_offline_train_target, feature_name = feature_name)\n#     valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values, label = df_offline_valid_target, feature_name = feature_name)\n\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    ctb_model = ctb.CatBoostRegressor(**ctb_params)\n#     summary = ctb_model.select_features(\n#         df_offline_train[feature_name], df_offline_train_target,\n#         eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n#         features_for_select=feature_name,\n#         num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n#         steps=3,\n#         algorithm=ctb.EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n#         shap_calc_type=ctb.EShapCalcType.Regular,\n#         train_final_model=False,\n#         plot=True,\n#     )\n    ctb_model.fit(\n        df_offline_train[feature_name_ctb], df_offline_train_target,\n        eval_set=[(df_offline_valid[feature_name_ctb], df_offline_valid_target)],\n        use_best_model=True,\n#         early_stopping_rounds=200\n    )\n    \n    best_iteration_ = ctb_model.best_iteration_\n    \n    pre[\"target_pre\"] = ctb_model.predict(df_offline_valid[feature_name_ctb])\n    pre[\"target_pre_pro\"] = pre.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target_pre\"].transform(lambda x: x - x.mean() + target_mean)\n    pre_ctb = pre[\"target_pre_pro\"].values\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, offline_split, ctb_model, pre\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n    # Adjust the number of estimators for the inference model\n    infer_params = ctb_params.copy()\n    infer_params[\"iterations\"] = int(1.2 * best_iteration_)\n    infer_ctb_model = ctb.CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[feature_name_ctb], df_train_target)\n    print(\"CatBoost training completed.\")\n    \n    return infer_ctb_model, pre_ctb, target_mean, df_offline_valid_target","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.876466Z","iopub.execute_input":"2023-12-19T23:08:57.876819Z","iopub.status.idle":"2023-12-19T23:08:57.891664Z","shell.execute_reply.started":"2023-12-19T23:08:57.876786Z","shell.execute_reply":"2023-12-19T23:08:57.890766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_weight(pre_lgb, pre_ctb, target_true):\n    def func(x):\n        return mean_absolute_error(pre_lgb * x + pre_ctb * (1 - x), target_true)\n    return golden_section_search(func, 0, 1, 1e-6)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.893069Z","iopub.execute_input":"2023-12-19T23:08:57.89346Z","iopub.status.idle":"2023-12-19T23:08:57.904727Z","shell.execute_reply.started":"2023-12-19T23:08:57.893428Z","shell.execute_reply":"2023-12-19T23:08:57.903838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(df_train_feats, targets, split_day):\n    df_train_feats['target'] = targets\n    df_train_feats.dropna(subset=[\"target\", \"wap\"], inplace=True)\n    targets = df_train_feats['target']\n    del df_train_feats['target']\n    gc.collect()\n    \n    infer_lgb_model, pre_lgb = train_lgb(df_train_feats, targets, split_day)\n    gc.collect()\n    infer_ctb_model, pre_ctb, target_mean, target_true = train_ctb(df_train_feats, targets, split_day)\n    gc.collect()\n    alpha = get_model_weight(pre_lgb, pre_ctb, target_true)\n    return infer_lgb_model, infer_ctb_model, alpha, target_mean\n#     return None, infer_ctb_model, 0, target_mean","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:08:57.907875Z","iopub.execute_input":"2023-12-19T23:08:57.908229Z","iopub.status.idle":"2023-12-19T23:08:57.917117Z","shell.execute_reply.started":"2023-12-19T23:08:57.908195Z","shell.execute_reply":"2023-12-19T23:08:57.916352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023, pickle, time\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\n# train_cache = pd.DataFrame()\nprint(\"Start initialization...\")\n\n# Process online\n# targets = df_train[\"target\"].astype(np.float32)\n# data = reduce_mem_usage(generate_all_features(df_train))\n\n# Load processed data\nwith open(\"/kaggle/input/optiver-final-dateset/df_feats.pkl\", \"rb\") as file:\n    data = pickle.load(file)\nwith open(\"/kaggle/input/optiver-training-data-features/df_target.pkl\", \"rb\") as file:\n    targets = pickle.load(file)\nprint(\"Loaded temporary data.\")\ndel df_train\n\n## Stress test\n# split = int(len(data) * 0.375)  # 660 / 480 - 1 = 0.375\n# data_dummy = data[:split].copy()\n# data_dummy[\"date_id\"] += 481\n# data = pd.concat([data, data_dummy], ignore_index=True, axis=0)\n# targets = pd.concat([targets, targets[:split]], ignore_index=True, axis=0)\n# del data_dummy\n##\n\ngc.collect()\nprint(\"Initialization done.\")\nstart_date_id = 481  # 481\nstart_pro_date_id = start_date_id - 1\n# train_date_id = set([start_date_id, start_date_id + 20, start_date_id + 40, start_date_id + 60, start_date_id + 80, start_date_id + 100])  # set([start_date_id + 20])\ntrain_date_id = set([start_date_id + 37])  # set([start_date_id + 20])\n# feature_name = list(data.columns)\n# feature_name.remove(\"date_id\")\n# feature_name_ctb = [feat for feat in feature_name if feat not in eliminated_features_names]\nfeat_last = None\nfeat = None\ntarget_last = None\ntarget_mean = targets.values.mean()\ninfer_lgb_model = joblib.load(\"/kaggle/input/optiver-480-model/lgbm.model\")  #None\ninfer_ctb_model = joblib.load(\"/kaggle/input/optiver-480-model/ctb.model\")   #None\nalpha = 0.52\ntest_float_columns = ['imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap',]\ntest_int_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag']\nrevealed_int_columns = ['stock_id', 'seconds_in_bucket']\nlast_day_id = 477\ntrain_cnt = 0\nprint(\"Start prediction...\")\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    \n    test[test_float_columns] = test[test_float_columns].astype(float)\n    test[test_int_columns] = test[test_int_columns].astype(int)\n    \n    currently_scored = test.iloc[0][\"currently_scored\"]\n    date_id = test[\"date_id\"].max()\n\n    # currently_scored = date_id >= start_date_id\n#     seconds_in_bucket = test.iloc[0][\"seconds_in_bucket\"]\n#     if seconds_in_bucket == 0:\n#         print(date_id)\n        \n    if date_id < start_pro_date_id:\n        last_day_id = date_id\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    del test['currently_scored']\n    \n    # Update cache and data for training\n#     if date_id >= start_date_id and len(revealed_targets) > 1:\n    if date_id >= start_date_id and last_day_id != date_id:\n        # Update `feat_last` and `target_last`\n        if feat is None:\n            feat = generate_all_features(cache, feat_last, target_last)\n        feat_last = feat\n        feat = None\n#         target_last = revealed_targets[\"revealed_target\"].values.astype(np.float32) \n        # drop duplicates\n        feat_last.drop_duplicates(['stock_id', 'seconds_in_bucket'], inplace=True)\n        revealed_targets.drop_duplicates(['stock_id', 'seconds_in_bucket'], inplace=True)\n        revealed_targets[revealed_int_columns] = revealed_targets[revealed_int_columns].astype(int)\n\n        target_last = feat_last.merge(revealed_targets[['stock_id', 'seconds_in_bucket', 'revealed_target']], on=['stock_id', 'seconds_in_bucket'], how='left')['revealed_target'].values.astype(np.float32) \n        cache = pd.DataFrame()\n        # If lastday's date_id is greater than start_date_id, update `data` and `targets` \n        if train_cnt < len(train_date_id) and date_id - 1 >= start_date_id:\n            data = pd.concat([data, reduce_mem_usage(feat_last)], ignore_index=True, axis=0)\n            targets = pd.concat([targets, pd.Series(target_last)], ignore_index=True, axis=0)\n        # Online train\n        if date_id in train_date_id:\n            del infer_lgb_model, infer_ctb_model\n            infer_lgb_model, infer_ctb_model, alpha, target_mean = train_model(data, targets, date_id - 1)\n            train_cnt += 1\n            # Clear `data` and `targets`, since no need to train again\n            if train_cnt >= len(train_date_id):\n                del data, targets\n    \n    last_day_id = date_id\n    # Generate features\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    \n    if not currently_scored:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    \n    feat = generate_all_features(cache, feat_last, target_last)\n    feat_cur = feat[-len(test):]\n    lgb_prediction = infer_lgb_model.predict(feat_cur[feature_name_lgb])\n    ctb_prediction = infer_ctb_model.predict(feat_cur[feature_name_ctb])\n    prediction = lgb_prediction * alpha + ctb_prediction * (1 - alpha)\n#     prediction = infer_ctb_model.predict(feat_cur[feature_name_ctb])\n    prediction = prediction - prediction.mean() + target_mean\n    clipped_predictions = np.clip(prediction, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\nprint(\"OK\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:10:28.146917Z","iopub.execute_input":"2023-12-19T23:10:28.147418Z","iopub.status.idle":"2023-12-19T23:11:08.813628Z","shell.execute_reply.started":"2023-12-19T23:10:28.147381Z","shell.execute_reply":"2023-12-19T23:11:08.812699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}