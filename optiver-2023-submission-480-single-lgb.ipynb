{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":7249830,"sourceType":"datasetVersion","datasetId":4200347},{"sourceId":155837852,"sourceType":"kernelVersion"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n\n# ðŸ“¦ Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport xgboost as xgb\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport catboost as ctb\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\n# from pandarallel import pandarallel\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# ðŸ¤ Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pandarallel.initialize(nb_workers=4)\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# ðŸ“Š Define flags and variables\n# is_offline = True  # Flag for online/offline mode\n# is_train = False  # Flag for training mode\n# is_infer = False  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\nN_STOCKS = 200\nMAX_N_NEIGHBOURS = 10\nNEIGHBOUR_CORR_THRESHOLD = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:12.894202Z","iopub.execute_input":"2023-12-20T19:45:12.894489Z","iopub.status.idle":"2023-12-20T19:45:17.806178Z","shell.execute_reply.started":"2023-12-20T19:45:12.894463Z","shell.execute_reply":"2023-12-20T19:45:17.805253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ðŸ“‚ Read the dataset from a CSV file using Pandas\n# df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# ðŸ§¹ Remove rows with missing values in the \"target\" column\ndf = df.dropna(subset=[\"target\", \"wap\"])\n\n# ðŸ” Reset the index of the DataFrame and apply the changes in place\ndf.reset_index(drop=True, inplace=True)\n\n# ðŸ“ Get the shape of the DataFrame (number of rows and columns)\ndf_shape = df.shape\n\ndf_train = df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:17.807961Z","iopub.execute_input":"2023-12-20T19:45:17.80827Z","iopub.status.idle":"2023-12-20T19:45:35.68012Z","shell.execute_reply.started":"2023-12-20T19:45:17.808243Z","shell.execute_reply":"2023-12-20T19:45:35.679205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ðŸ§¹ Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # ðŸ“ Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # ðŸ”„ Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # â„¹ï¸ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # ðŸ”„ Return the DataFrame with optimized memory usage\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:35.681257Z","iopub.execute_input":"2023-12-20T19:45:35.681518Z","iopub.status.idle":"2023-12-20T19:45:35.695121Z","shell.execute_reply.started":"2023-12-20T19:45:35.681495Z","shell.execute_reply":"2023-12-20T19:45:35.694061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ðŸŽï¸ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# ðŸ“Š Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # ðŸ” Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # ðŸ” Loop through rows of the DataFrame\n        for j in range(num_rows):\n\n            if df_values[j, a] < df_values[j, b]:\n                min_val = df_values[j, a]\n                max_val = df_values[j, b]\n            else:\n                max_val = df_values[j, a]\n                min_val = df_values[j, b]\n\n            if min_val < df_values[j, c]:\n                if df_values[j, c] < max_val:\n                    mid_val = df_values[j, c]\n                else:\n                    mid_val = max_val\n                    max_val = df_values[j, c]\n            else:\n                mid_val = min_val\n                min_val = df_values[j, c]\n            \n            # ðŸš« Prevent division by zero\n            if max_val == min_val:\n                imbalance_features[j, i] = np.nan\n            elif mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    \n    return imbalance_features\n\n# ðŸ“ˆ Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:35.697305Z","iopub.execute_input":"2023-12-20T19:45:35.697596Z","iopub.status.idle":"2023-12-20T19:45:36.314125Z","shell.execute_reply.started":"2023-12-20T19:45:35.697562Z","shell.execute_reply":"2023-12-20T19:45:36.313249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ðŸ“Š Function to generate imbalance features\ndef imbalance_features(df):\n\n    stock_groups = df.groupby([\"date_id\", \"seconds_in_bucket\"])\n    # Index WAP\n    df[\"wwap\"] = df.stock_id.map(weights) * df.wap\n    df[\"iwap\"] = stock_groups[\"wwap\"].transform(lambda x: x.sum())\n    del df[\"wwap\"]\n\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"all_size\"] = df.eval(\"matched_size + imbalance_size\") # add\n    df[\"imbalance_size_for_buy_sell\"] = df.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\n    # df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    cols = ['wap', 'imbalance_size_for_buy_sell', \"bid_size\", \"ask_size\"]\n    for q in [0.25, 0.5, 0.75]:  # Try more/different q\n        df[[f'all_{col}_quantile_{q}' for col in cols]] = stock_groups[cols].transform(lambda x: x.quantile(q)).astype(np.float32)\n        \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\").astype(np.float32)\n\n    for c in combinations(sizes, 2):\n        df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\").astype(np.float32)\n\n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values.astype(np.float32)\n        \n    # V2 features\n    # Calculate additional features\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    df[\"imbalance_momentum\"] = stock_groups['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = stock_groups['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['wap_advantage'] = df.wap - df.iwap  # add\n\n    # Calculate various statistical aggregation features\n    df_prices = df[prices]\n    df_sizes = df[sizes]\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df_prices.agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df_sizes.agg(func, axis=1)\n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    cols = ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_shift_{window}\" for col in cols]] = stock_groups_cols.shift(window)\n\n    cols = ['matched_size', 'imbalance_size', 'reference_price', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_ret_{window}\" for col in cols]] = stock_groups_cols.pct_change(window).astype(np.float32)\n\n    # Calculate diff features for specific columns\n    cols = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price', 'imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_diff_{window}\" for col in cols]] = stock_groups_cols.diff(window).astype(np.float32)\n\n    # V4 features\n    # Construct `time_since_last_imbalance_change`\n    df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n    # ä½¿ç”¨cumsumåˆ›å»ºä¸€ä¸ªç»„æ ‡è¯†ç¬¦ï¼Œæ¯å½“flagæ”¹å˜æ—¶ï¼Œç»„æ ‡è¯†ç¬¦å¢žåŠ \n    df['group'] = df.groupby(['stock_id', 'date_id'])['flag_change'].cumsum()\n    # å¯¹æ¯ä¸ªç»„å†…çš„'seconds_in_bucket'è®¡ç®—æ—¶é—´å·®ï¼Œä»¥å¾—åˆ°è‡ªä¸Šæ¬¡flagæ”¹å˜ä»¥æ¥çš„æ—¶é—´\n    group_min = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform('min')\n    df['time_since_last_imbalance_change'] = df['seconds_in_bucket'] - group_min\n    # `flag_change`ä¸º1çš„åœ°æ–¹è®¾ä¸º0\n    df['time_since_last_imbalance_change'] *= (1 - df['flag_change'])\n    df.drop(columns=['flag_change', 'group'], inplace=True)\n    \n    cols = ['imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [5, 10]:\n        mean_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).mean())\n        std_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).std())\n        df[[f'z_score_{col}_{window}' for col in cols]] = (df[cols] - mean_col) / std_col\n    \n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\n# ðŸ“… Function to generate time and stock-related features\ndef other_features(df):\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n        \n#     for key, value in global_seconds_feats.items():\n#         df[f\"global_seconds_{key}\"] = df[\"seconds_in_bucket\"].map(value.to_dict())\n\n    return df\n\ndef last_days_features(df: pd.DataFrame, feat_last=None, target_last=None):\n    size = None\n    \n    if feat_last is not None and len(feat_last) > 0:\n        cols = [col for col in df.columns if col in set(feat_last.columns)]\n        if target_last is not None:\n            cols.append(\"target\")\n            feat_last[\"target\"] = target_last\n            df[\"target\"] = 0\n        paddings = []\n        second_start = df.seconds_in_bucket.max()\n        padding_src = df[df.seconds_in_bucket == second_start]\n        size = len(df)\n        size_pad = len(padding_src) * 6\n        for second in range(second_start + 10, second_start + 70, 10):\n            padding = padding_src.copy()\n            padding[\"seconds_in_bucket\"] = second\n            paddings.append(padding)\n        df = pd.concat([feat_last[cols], df] + paddings)\n\n    # Add Last days features\n    # TODO: Try more features\n    cols = ['near_price', 'far_price', 'depth_pressure']\n    if 'target' in df.columns:\n        cols.append('target')\n    stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1]:\n        df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n    if cols[-1] == \"target\":\n        cols.pop()\n    \n    cols = [f\"{col}_last_{window}day\" for col in cols]\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6]:\n        df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n        \n    if size:\n        return df[-(size + size_pad):-size_pad]\n    return df\n\n# ðŸš€ Function to generate all features by combining imbalance and other features\ndef generate_all_features(df, feat_last=None, target_last=None):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in {\"row_id\", \"time_id\", \"currently_scored\"}]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    \n    # Generate last days features\n    df = last_days_features(df, feat_last, target_last)\n    \n    # Generate time and stock-related features\n    df = other_features(df)\n\n    gc.collect()  # Perform garbage collection to free up memory\n    libc.malloc_trim(0)\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in {\"row_id\", \"target\", \"time_id\"}]\n    \n    return df[feature_name]","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:36.315795Z","iopub.execute_input":"2023-12-20T19:45:36.316156Z","iopub.status.idle":"2023-12-20T19:45:36.354962Z","shell.execute_reply.started":"2023-12-20T19:45:36.316127Z","shell.execute_reply":"2023-12-20T19:45:36.354013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}\n\ndf_train[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # add\nglobal_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n        \"median_far_price\": df_train.groupby(\"stock_id\")[\"far_price\"].median(),\n        \"median_near_price\": df_train.groupby(\"stock_id\")[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": df_train.groupby(\"stock_id\")[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\": df_train.groupby(\"stock_id\")[\"matched_size\"].median(),\n    }\ndel df_train[\"imbalance_size_for_buy_sell\"]\n\n# global_seconds_feats = {\n#     \"median_target\": df_train.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n# }\n# stock2nei = get_stock_neighbour(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:36.356304Z","iopub.execute_input":"2023-12-20T19:45:36.356602Z","iopub.status.idle":"2023-12-20T19:45:38.937435Z","shell.execute_reply.started":"2023-12-20T19:45:36.356576Z","shell.execute_reply":"2023-12-20T19:45:38.936627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_lgb(df_train_feats, targets, split_day):\n    \n    # stress test\n#     split_day = df_train_feats.date_id.max()\n    target_mean = targets.mean()\n    \n    # Update global features\n    global global_stock_id_feats, feature_name\n    global_stock_id_feats = {\n        \"median_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].median() + df_train_feats.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].std() + df_train_feats.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train_feats.groupby(\"stock_id\")[\"bid_size\"].max() - df_train_feats.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].median() + df_train_feats.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].std() + df_train_feats.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train_feats.groupby(\"stock_id\")[\"bid_price\"].max() - df_train_feats.groupby(\"stock_id\")[\"ask_price\"].min(),\n        \"median_far_price\": df_train_feats.groupby(\"stock_id\")[\"far_price\"].median(),\n        \"median_near_price\": df_train_feats.groupby(\"stock_id\")[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": df_train_feats.groupby(\"stock_id\")[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\": df_train_feats.groupby(\"stock_id\")[\"matched_size\"].median(),\n    }\n#     df_train_feats[\"target\"] = targets.values\n#     global_seconds_feats = {\n#         \"median_target\": df_train_feats.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n#     }\n#     del df_train_feats[\"target\"]\n    \n    # update global features\n    for key, value in global_stock_id_feats.items():\n        df_train_feats[f\"global_{key}\"] = df_train_feats[\"stock_id\"].map(value.to_dict())\n        \n#     for key, value in global_seconds_feats.items():\n#         df_train_feats[f\"global_seconds_{key}\"] = df_train_feats[\"seconds_in_bucket\"].map(value.to_dict())\n        \n#     # Limit N newest data to avoid OOM\n#     keep_days = 500\n#     data_mask = df_train_feats.date_id >= split_day + 1 - keep_days\n# #     keep_rows = 6274400 # 600: 6557200; 575: 6274400\n#     keep_rows = data_mask.sum()\n#     df_train_feats = df_train_feats[-keep_rows:]\n#     targets = targets[-keep_rows:]\n#     del data_mask\n#     gc.collect()\n\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n#     pre = df_offline_valid[[\"date_id\", \"seconds_in_bucket\"]]\n#     pre[\"target\"] = df_offline_valid_target.values\n        \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5000,\n        \"num_leaves\": 465,\n        \"subsample\": 0.65791,\n        \"colsample_bytree\": 0.7,\n        \"learning_rate\": 0.00877,  # 0.00877\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 14,  # Maximum depth of the tree\n        \"min_child_samples\": 132,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 6,  # L1 regularization term\n        \"reg_lambda\": 0.08,  # L2 regularization term\n    }\n\n    print(f\"Feature length = {len(feature_name)}\")\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_offline_train[feature_name].values.astype(np.float32),\n        df_offline_train_target.values.astype(np.float32),\n        eval_set=[(df_offline_valid[feature_name].values.astype(np.float32), df_offline_valid_target.values.astype(np.float32))],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=200),\n            lgb.callback.log_evaluation(period=100),\n        ],\n        feature_name = feature_name\n    )\n    \n    best_iteration_ = lgb_model.best_iteration_\n    \n#     pre[\"target_pre\"] = lgb_model.predict(df_offline_valid[feature_name])\n#     pre[\"target_pre_pro\"] = pre.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target_pre\"].transform(lambda x: x - x.mean() + target_mean)\n#     pre_lgb = pre[\"target_pre_pro\"].values\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split, lgb_model\n    gc.collect()\n    libc.malloc_trim(0)\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = lgb_params.copy()\n    infer_params[\"n_estimators\"] = int(1.2 * best_iteration_)\n    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n    infer_lgb_model.fit(\n        df_train_feats[feature_name].values.astype(np.float32), \n        df_train_target.values.astype(np.float32), \n        feature_name = feature_name\n        )\n    print(\"LightGBM training completed.\")\n    return infer_lgb_model, target_mean","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:38.938917Z","iopub.execute_input":"2023-12-20T19:45:38.939214Z","iopub.status.idle":"2023-12-20T19:45:38.957222Z","shell.execute_reply.started":"2023-12-20T19:45:38.939189Z","shell.execute_reply":"2023-12-20T19:45:38.95623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(\"/kaggle/input/optiver-training-data-features/df_feat.pkl\", \"rb\") as file:\n    data = pickle.load(file)\nwith open(\"/kaggle/input/optiver-training-data-features/df_target.pkl\", \"rb\") as file:\n    targets = pickle.load(file)\nprint(\"Loaded temporary data.\")\ndel df_train","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:45:38.958295Z","iopub.execute_input":"2023-12-20T19:45:38.958536Z","iopub.status.idle":"2023-12-20T19:45:39.211707Z","shell.execute_reply.started":"2023-12-20T19:45:38.958514Z","shell.execute_reply":"2023-12-20T19:45:39.210741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\nprint(\"Start initialization...\")\nprint(\"Initialization done.\")\nstart_date_id = 481  # 481\nstart_pro_date_id = start_date_id - 1\n# train_date_id = set([start_date_id, start_date_id + 20, start_date_id + 40, start_date_id + 60, start_date_id + 80, start_date_id + 100])  # set([start_date_id + 20])\ntrain_date_id = set()  # set([start_date_id + 20])\nfeature_name = list(data.columns)\nfeature_name.remove(\"date_id\")\ngc.collect()\nlibc.malloc_trim(0)\n# feature_name_ctb = [feat for feat in feature_name if feat not in eliminated_features_names]\nfeat_last = None\nfeat = None\ntarget_last = None\ntarget_mean = targets.mean()\ninfer_lgb_model = joblib.load(\"/kaggle/input/final-lgb-480-model/single_lgbm.model\")  #None\n# infer_ctb_model = None\n# alpha = 0.67\ntest_float_columns = ['imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap',]\ntest_int_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag']\nrevealed_int_columns = ['stock_id', 'seconds_in_bucket']\nlast_day_id = 477\ntrain_cnt = 0\nprint(\"Start prediction...\")\n\n\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    \n    test[test_float_columns] = test[test_float_columns].astype(float)\n    test[test_int_columns] = test[test_int_columns].astype(int)\n    \n    currently_scored = test.iloc[0][\"currently_scored\"]\n    date_id = test[\"date_id\"].max()\n    seconds_in_bucket = test[\"seconds_in_bucket\"].max()\n    \n    test[\"date_id\"] = date_id\n    test[\"seconds_in_bucket\"] = seconds_in_bucket\n        \n    if date_id < start_pro_date_id:\n        last_day_id = date_id\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    del test['currently_scored']\n    \n    # Update cache and data for training\n#     if date_id >= start_date_id and len(revealed_targets) > 1:\n    if date_id >= start_date_id and last_day_id != date_id:\n        # Update `feat_last` and `target_last`\n        if feat is None:\n            feat = generate_all_features(cache, feat_last, target_last)\n        feat_last = feat\n        feat = None\n        # drop duplicates\n        feat_last.drop_duplicates(['stock_id', 'seconds_in_bucket'], inplace=True)\n        revealed_targets.drop_duplicates(['stock_id', 'seconds_in_bucket'], inplace=True)\n        revealed_targets[revealed_int_columns] = revealed_targets[revealed_int_columns].astype(int)\n        target_last = feat_last.merge(revealed_targets[['stock_id', 'seconds_in_bucket', 'revealed_target']], on=['stock_id', 'seconds_in_bucket'], how='left')['revealed_target'].values.astype(np.float32) \n        cache = pd.DataFrame()\n\n    last_day_id = date_id\n    # Generate features\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    \n    if not currently_scored:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n        \n    feat = generate_all_features(cache, feat_last, target_last)\n    feat_cur = feat[-len(test):]\n    prediction = infer_lgb_model.predict(feat_cur[feature_name])\n    prediction = prediction - prediction.mean() + target_mean\n    clipped_predictions = np.clip(prediction, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\nprint(\"OK\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}